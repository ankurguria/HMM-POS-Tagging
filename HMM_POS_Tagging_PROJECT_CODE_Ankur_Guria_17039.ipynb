{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANKUR GURIA : 17039 : DSE 308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and Packages\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\ANKUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\ANKUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Downloading the treebank corpus from NLTK\n",
    "nltk.download('treebank')\n",
    " \n",
    "# Downloading the universal tagset from NLTK\n",
    "nltk.download('universal_tagset')\n",
    " \n",
    "# Reading the Treebank tagged sentences\n",
    "nltk_tagged_sentences = list(nltk.corpus.treebank.tagged_sents(tagset = 'universal'))\n",
    "\n",
    "# Printing the first two sentences along with tags\n",
    "print(nltk_tagged_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pierre', 'NOUN')\n",
      "('Vinken', 'NOUN')\n",
      "(',', '.')\n",
      "('61', 'NUM')\n",
      "('years', 'NOUN')\n",
      "('old', 'ADJ')\n",
      "(',', '.')\n",
      "('will', 'VERB')\n",
      "('join', 'VERB')\n",
      "('the', 'DET')\n",
      "('board', 'NOUN')\n",
      "('as', 'ADP')\n",
      "('a', 'DET')\n",
      "('nonexecutive', 'ADJ')\n",
      "('director', 'NOUN')\n",
      "('Nov.', 'NOUN')\n",
      "('29', 'NUM')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "# Printing each word with its respective tag for the first sentence\n",
    "for sentence in nltk_tagged_sentences[:1]: \n",
    "    for tuple in sentence: print(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into training and validation set in the ratio 85:15\n",
    "train_data, test_data = train_test_split(nltk_tagged_sentences, train_size = 0.85, test_size = 0.15, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of train and test tagged words\n",
    "train_words, test_words = [], []\n",
    "\n",
    "for sentence in train_data:\n",
    "    for tuple in sentence:\n",
    "        train_words.append(tuple)        \n",
    "for sentence in test_data:\n",
    "    for tuple in sentence:\n",
    "        test_words.append(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Next', 'ADJ'),\n",
       " ('week', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('Philippine', 'NOUN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the tagged words\n",
    "train_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'.', 'VERB', 'CONJ', 'ADJ', 'PRON', 'DET', 'ADP', 'ADV', 'NUM', 'PRT', 'NOUN', 'X'}\n",
      "11394\n"
     ]
    }
   ],
   "source": [
    "# Unique tags in training data\n",
    "all_tags = []\n",
    "for word, tag in train_words: all_tags.append(tag)\n",
    "unique_tags = set(all_tags)\n",
    "print(len(unique_tags))\n",
    "print(unique_tags)\n",
    " \n",
    "# Unique words in vocabulary of training data\n",
    "all_words = []\n",
    "for word, tag in train_words: all_words.append(word)\n",
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EMISSION PROBABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_prob(word, tag, data = train_words):\n",
    "    list_tag = [tuple for tuple in data if tuple[1] == tag]\n",
    "    tag_count = len(list_tag) # count of the number of times the current tag occurred in the data\n",
    "    word_given_list_tag = [tuple[0] for tuple in list_tag if tuple[0] == word]\n",
    "    count_word_given_tag = len(word_given_list_tag) # count of the number of times the current word occurred as the passed tag.\n",
    " \n",
    "    return count_word_given_tag / tag_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRANSITION PROBABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_prob(tag2, tag1, data = train_words):\n",
    "    tags = [tuple[1] for tuple in data]\n",
    "    count_tag1 = len([t for t in tags if t == tag1])\n",
    "    count_tag2_tag1 = 0\n",
    "    for ind in range(len(tags)-1):\n",
    "        if tags[ind] == tag1 and tags[ind + 1] == tag2: count_tag2_tag1 += 1\n",
    "    return count_tag2_tag1 / count_tag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.08354446e-02 9.04303789e-02 5.91392405e-02 4.52658236e-02\n",
      "  6.72405064e-02 1.72658235e-01 9.21518952e-02 5.18481024e-02\n",
      "  7.89873451e-02 2.63291132e-03 2.21974686e-01 2.67341770e-02]\n",
      " [3.44767682e-02 1.68562740e-01 5.47112478e-03 6.61745518e-02\n",
      "  3.55188884e-02 1.33738607e-01 9.24880579e-02 8.34563598e-02\n",
      "  2.31871475e-02 3.02214511e-02 1.09943554e-01 2.16760740e-01]\n",
      " [3.52514274e-02 1.50336966e-01 5.18403307e-04 1.15085535e-01\n",
      "  5.75427674e-02 1.22861587e-01 5.65059632e-02 5.75427674e-02\n",
      "  3.83618437e-02 4.66562994e-03 3.51995856e-01 9.33125988e-03]\n",
      " [6.56008795e-02 1.15766264e-02 1.67217944e-02 6.48658574e-02\n",
      "  1.83755968e-04 5.14516700e-03 7.99338445e-02 4.96141147e-03\n",
      "  2.16832049e-02 1.12091145e-02 6.97353899e-01 2.07644254e-02]\n",
      " [4.20095287e-02 4.85058457e-01 5.19705517e-03 7.23256841e-02\n",
      "  6.49631862e-03 9.09484643e-03 2.25205719e-02 3.55132110e-02\n",
      "  7.36249471e-03 1.34257255e-02 2.14378521e-01 8.66175815e-02]\n",
      " [1.74962711e-02 3.96039598e-02 5.42520022e-04 2.05615088e-01\n",
      "  3.25512001e-03 5.69646014e-03 9.62973014e-03 1.17998105e-02\n",
      "  2.27858406e-02 2.71260011e-04 6.38003528e-01 4.53004204e-02]\n",
      " [3.84661332e-02 8.48166272e-03 9.55680327e-04 1.07275113e-01\n",
      "  6.92868233e-02 3.22183728e-01 1.67244058e-02 1.42157450e-02\n",
      "  6.35527447e-02 1.19460037e-03 3.23139399e-01 3.45239528e-02]\n",
      " [1.39629632e-01 3.40000004e-01 7.03703705e-03 1.30370364e-01\n",
      "  1.29629625e-02 7.11111128e-02 1.18888892e-01 8.03703740e-02\n",
      "  3.07407416e-02 1.48148146e-02 3.11111119e-02 2.29629632e-02]\n",
      " [1.18712276e-01 2.01207250e-02 1.37491617e-02 3.52112688e-02\n",
      "  1.67672709e-03 3.68879945e-03 3.62173021e-02 3.35345417e-03\n",
      "  1.81757212e-01 2.61569414e-02 3.55130792e-01 2.04225346e-01]\n",
      " [4.42477874e-02 4.05235976e-01 2.21238937e-03 8.14896747e-02\n",
      "  1.73303839e-02 9.95575190e-02 1.99115053e-02 9.95575264e-03\n",
      "  5.71533926e-02 1.10619469e-03 2.49262542e-01 1.25368731e-02]\n",
      " [2.39839554e-01 1.48528636e-01 4.25653830e-02 1.24831172e-02\n",
      "  4.54303622e-03 1.30151846e-02 1.76605403e-01 1.66168697e-02\n",
      "  9.37256962e-03 4.36295159e-02 2.63659805e-01 2.91409157e-02]\n",
      " [1.61621615e-01 2.05225229e-01 1.08108111e-02 1.67567562e-02\n",
      "  5.53153157e-02 5.62162176e-02 1.42522529e-01 2.54054051e-02\n",
      "  3.06306314e-03 1.87027022e-01 6.05405420e-02 7.54954964e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Constructing a t x t transition matrix of tags wher t is equal to the nunmber of unique tags\n",
    "# Tag_Matrix(i, j) represents Probability of the jth index tag occurring after the ith index tag\n",
    " \n",
    "tag_matrix = np.zeros((len(unique_tags), len(unique_tags)), dtype = 'float32')\n",
    "for i, tag1 in enumerate(list(unique_tags)):\n",
    "    for j, tag2 in enumerate(list(unique_tags)): tag_matrix[i, j] = transition_prob(tag2, tag1)\n",
    " \n",
    "print(tag_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>VERB</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.090835</td>\n",
       "      <td>0.090430</td>\n",
       "      <td>0.059139</td>\n",
       "      <td>0.045266</td>\n",
       "      <td>0.067241</td>\n",
       "      <td>0.172658</td>\n",
       "      <td>0.092152</td>\n",
       "      <td>0.051848</td>\n",
       "      <td>0.078987</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.221975</td>\n",
       "      <td>0.026734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.034477</td>\n",
       "      <td>0.168563</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.066175</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>0.133739</td>\n",
       "      <td>0.092488</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.030221</td>\n",
       "      <td>0.109944</td>\n",
       "      <td>0.216761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.150337</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.115086</td>\n",
       "      <td>0.057543</td>\n",
       "      <td>0.122862</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>0.057543</td>\n",
       "      <td>0.038362</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.351996</td>\n",
       "      <td>0.009331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.065601</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.064866</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.079934</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.021683</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.697354</td>\n",
       "      <td>0.020764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.042010</td>\n",
       "      <td>0.485058</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.022521</td>\n",
       "      <td>0.035513</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>0.214379</td>\n",
       "      <td>0.086618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.017496</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.205615</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.638004</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.107275</td>\n",
       "      <td>0.069287</td>\n",
       "      <td>0.322184</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>0.063553</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.323139</td>\n",
       "      <td>0.034524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.139630</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.130370</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>0.080370</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.022963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.118712</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.036217</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.181757</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>0.355131</td>\n",
       "      <td>0.204225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.405236</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.081490</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>0.099558</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.057153</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.249263</td>\n",
       "      <td>0.012537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.239840</td>\n",
       "      <td>0.148529</td>\n",
       "      <td>0.042565</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.176605</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.263660</td>\n",
       "      <td>0.029141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.161622</td>\n",
       "      <td>0.205225</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.016757</td>\n",
       "      <td>0.055315</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.142523</td>\n",
       "      <td>0.025405</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.187027</td>\n",
       "      <td>0.060541</td>\n",
       "      <td>0.075495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .      VERB      CONJ       ADJ      PRON       DET       ADP  \\\n",
       ".     0.090835  0.090430  0.059139  0.045266  0.067241  0.172658  0.092152   \n",
       "VERB  0.034477  0.168563  0.005471  0.066175  0.035519  0.133739  0.092488   \n",
       "CONJ  0.035251  0.150337  0.000518  0.115086  0.057543  0.122862  0.056506   \n",
       "ADJ   0.065601  0.011577  0.016722  0.064866  0.000184  0.005145  0.079934   \n",
       "PRON  0.042010  0.485058  0.005197  0.072326  0.006496  0.009095  0.022521   \n",
       "DET   0.017496  0.039604  0.000543  0.205615  0.003255  0.005696  0.009630   \n",
       "ADP   0.038466  0.008482  0.000956  0.107275  0.069287  0.322184  0.016724   \n",
       "ADV   0.139630  0.340000  0.007037  0.130370  0.012963  0.071111  0.118889   \n",
       "NUM   0.118712  0.020121  0.013749  0.035211  0.001677  0.003689  0.036217   \n",
       "PRT   0.044248  0.405236  0.002212  0.081490  0.017330  0.099558  0.019912   \n",
       "NOUN  0.239840  0.148529  0.042565  0.012483  0.004543  0.013015  0.176605   \n",
       "X     0.161622  0.205225  0.010811  0.016757  0.055315  0.056216  0.142523   \n",
       "\n",
       "           ADV       NUM       PRT      NOUN         X  \n",
       ".     0.051848  0.078987  0.002633  0.221975  0.026734  \n",
       "VERB  0.083456  0.023187  0.030221  0.109944  0.216761  \n",
       "CONJ  0.057543  0.038362  0.004666  0.351996  0.009331  \n",
       "ADJ   0.004961  0.021683  0.011209  0.697354  0.020764  \n",
       "PRON  0.035513  0.007362  0.013426  0.214379  0.086618  \n",
       "DET   0.011800  0.022786  0.000271  0.638004  0.045300  \n",
       "ADP   0.014216  0.063553  0.001195  0.323139  0.034524  \n",
       "ADV   0.080370  0.030741  0.014815  0.031111  0.022963  \n",
       "NUM   0.003353  0.181757  0.026157  0.355131  0.204225  \n",
       "PRT   0.009956  0.057153  0.001106  0.249263  0.012537  \n",
       "NOUN  0.016617  0.009373  0.043630  0.263660  0.029141  \n",
       "X     0.025405  0.003063  0.187027  0.060541  0.075495  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tag_Matrix as a Dataframe\n",
    "tag_df = pd.DataFrame(tag_matrix, columns = list(unique_tags), index = list(unique_tags))\n",
    "display(tag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VITERBI ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, data = train_words):\n",
    "    hidden_state = []\n",
    "    T = list(set([tuple[1] for tuple in data]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        p = [] #initialising list of probability column for a given observation\n",
    "        for tag in T:\n",
    "            if key == 0: transition_probability = tag_df.loc['.', tag]\n",
    "            else: transition_probability = tag_df.loc[hidden_state[-1], tag]\n",
    "                 \n",
    "            # computing emission and hidden_state probabilities\n",
    "            emission_probability = emission_prob(words[key], tag)\n",
    "            hidden_state_probability = emission_probability * transition_probability    \n",
    "            p.append(hidden_state_probability)\n",
    "             \n",
    "        prob_max = max(p)\n",
    "        \n",
    "        hidden_state_max = T[p.index(prob_max)] # getting hidden_state for which probability is maximum\n",
    "        hidden_state.append(hidden_state_max)\n",
    "        \n",
    "    return list(zip(words, hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Viterbi algorithm on few sentences of test dataset\n",
    "random.seed(1234)      #define a random seed to get same sentences when run multiple times\n",
    " \n",
    "rndom = [random.randint(1, len(test_data)) for x in range(10)] # choose random 10 numbers\n",
    "test_run = [test_data[i] for i in rndom] # list of 10 sents on which we test the model\n",
    "\n",
    "test_run_base = [tup for sent in test_run for tup in sent] # list of tagged words\n",
    "test_words = [tup[0] for sent in test_run for tup in sent] # list of untagged words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VITERBI ALGORIHTM ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Algorithm Accuracy:  91.32420091324201\n"
     ]
    }
   ],
   "source": [
    "# We are testing only on 10 sentences to check the accuracy as testing on the whole dataset will take a lot of time\n",
    "tagged_sequences = Viterbi(test_words)\n",
    "\n",
    "# accuracy\n",
    "counter = [res1 for res1, res2 in zip(tagged_sequences, test_run_base) if res1 == res2] \n",
    " \n",
    "model_accuracy = len(counter) / len(tagged_sequences)\n",
    "print('Viterbi Algorithm Accuracy: ', model_accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to test accuracy on all the test sentences\n",
    "##### CAUTION: Running the below cell takes a lot of time (You may want to avoid it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences()\n",
    "test_words = [tup for sent in test_data for tup in sent]\n",
    "test_untagged_words = [tup[0] for sent in test_data for tup in sent]\n",
    "\n",
    "tagged_seq = Viterbi(test_untagged_words)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(test_words, test_untagged_words) if i == j] \n",
    " \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Viterbi Algorithm Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to optimize the performance by specifying a rule base tagger for unknown words \n",
    "# Specifying rules for tagging unknown words\n",
    "rules = [\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'\\*T?\\*?-[0-9]+$', 'X'),        # X\n",
    "    (r'.*ed$', 'VERB'),               # past tense \n",
    "    (r'.*es$', 'VERB'),               # verb    \n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN')                   # nouns\n",
    "]\n",
    " \n",
    "# Building rule based tagger\n",
    "rule_tagger = nltk.RegexpTagger(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Viterbi to include rule based tagger\n",
    "def viterbi_rule_tagger(words, data = train_words):\n",
    "    hidden_state = []\n",
    "    T = list(set([tuple[1] for tuple in data]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        p = [] #initialising list of probability column for a given observation\n",
    "        for tag in T:\n",
    "            if key == 0: transition_probability = tag_df.loc['.', tag]\n",
    "            else: transition_probability = tag_df.loc[hidden_state[-1], tag]\n",
    "                 \n",
    "            # computing emission and hidden_state probabilities\n",
    "            emission_probability = emission_prob(words[key], tag)\n",
    "            hidden_state_probability = emission_probability * transition_probability   \n",
    "            p.append(hidden_state_probability)\n",
    "             \n",
    "        prob_max = max(p)\n",
    "        hidden_state_max = rule_tagger.tag([word])[0][1]       \n",
    "        \n",
    "        if(prob_max == 0): hidden_state_max = rule_tagger.tag([word])[0][1] # assign based on rule based tagger\n",
    "        else:\n",
    "            if hidden_state_max != 'X':hidden_state_max = T[p.index(prob_max)] # getting hidden_state for which probability is maximum\n",
    "             \n",
    "        hidden_state.append(hidden_state_max)\n",
    "        \n",
    "    return list(zip(words, hidden_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VITERBI ALGORIHTM WITH RULE BASED TAGGING ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Algorithm with rule based tagger Accuracy:  96.80365296803653\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on subset of test data \n",
    "tagged_sequences = viterbi_rule_tagger(test_words)\n",
    " \n",
    "# accuracy\n",
    "counter = [res1 for res1, res2 in zip(tagged_sequences, test_run_base) if res1 == res2] \n",
    " \n",
    "model_accuracy = len(counter) / len(tagged_sequences)\n",
    "print('Viterbi Algorithm with rule based tagger Accuracy: ', model_accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMPARING THE 2 POST TAGGERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Will', 'NOUN'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', 'NOUN')]\n",
      "[('Will', '.'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', '.')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Will can see Marry\"\n",
    "predicted_tag_rule = viterbi_rule_tagger(test_sentence.split())\n",
    "predicted_tag_no_rule = Viterbi(test_sentence.split())\n",
    "print(predicted_tag_rule)\n",
    "print(predicted_tag_no_rule)\n",
    "#Will and Marry are tagged as DET as they are unknown words for Viterbi Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
